{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering de Livres à partir d'une Base de Données d'Amazon\n",
    "##### AGIER Olivier - CHEVALIER Arnaud\n",
    "\n",
    "L'objectif de ce projet est de réaliser une opération de clustering sur les données brutes présentes dans une base de données du site Amazon.com . L'idée est de regrouper ces livres dans un certains nombre de clusters cohérents (livres d'un même auteur, titres similaires...), afin de pouvoir y appliquer différents algorithmes dans le cadre d'un projet d'option.\n",
    "\n",
    "A partr des données brutes de départ, on cherche à créer une connaissance de similarité entre les livres. Ce n'est cependant pas un système de recommandation. On ne cherche pas à trier les livres par pertinence par rappport à un ou plusieurs autres, mais bien à découper les `§INSERT_NUMBER` éléments du set de base en un certain nombre de clusters de taille équivalentes.\n",
    "\n",
    "Dans un premier temps, nous allons importer et \"nettoyer\" les données, nous découperons ensuite les titres et les auteurs en mots. Une pseudo distance sera ensuite définie afin d'appliquer une première clusterisation (grâce à la méthode k-means après réalisation d'une réduction de dimension) sur les mots présents dans les titres et les auteurs. Cette clusterisation permettra ensuite de réappliquer un k-means sur les vecteurs de mots correspondant aux entrées de la base de données initiale.\n",
    "\n",
    "Enfin nous vérifierons la qualité de la clusterisation finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 1 : Import et Nettoyage des données brutes\n",
    "\n",
    "La base de données que nous utiliserons `amazon_livres.txt` contient beaucoup d'informations, nous allons dans un premier temps nous limiter à celles qui nous intéressent : les titres des livres et les auteurs.\n",
    "\n",
    "L'ensemble des opérations de pré-processing pour récupérer les informations brutes initiales sont regroupées dans le dossier du même nom. Celles-ci ont été mises au point dans le cadre du projet d'option dans lequel s'inscrit ce projet. Nous ne rentrerons donc pas dans les détails ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"amazon_livres.txt\", sep=\"\\t\", dtype=str, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['ASIN', 'ISBN', 'TITLE', 'AUTHOR', 'PUBLISHER', 'PUBLICATION_DATE', 'PARUTION_ID']]\n",
    "data = data[data['AUTHOR'].notnull()]\n",
    "data = data[data['TITLE'].notnull()]\n",
    "data.rename({'AUTHOR': 'AUTEUR', 'TITLE': 'TITRE'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pre_processing.cleaning\n",
    "import pre_processing.volume_number_detection\n",
    "data['author'] = pre_processing.cleaning.clean_df_column(data.AUTEUR)\n",
    "data['title'] = pre_processing.cleaning.clean_df_column(data.TITRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(string):\n",
    "    output = str(string)\n",
    "    string = string.split()\n",
    "    output = output.split()\n",
    "    for word in string:\n",
    "        if len(word) <= 2:\n",
    "            output.remove(word)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['ASIN',\n",
    " 'ISBN',\n",
    " 'TITRE',\n",
    " 'AUTEUR',\n",
    " 'PUBLISHER',\n",
    " 'PUBLICATION_DATE',\n",
    " 'PARUTION_ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce stade, les données sont de la forme §INSERT_EXAMPLE . L'ensemble des éléments représente `§INSERT_NUMBER` lignes dans la table `data`.\n",
    "\n",
    "Nous allons par la suite nous intéresser à chaque mot des différentes listes et compter son nombre de connexions avec les autres. Les mots de deux lettre ou moins (\"LA\", \"DE\", \"OU\", \"D.\", ...) auront à l'évidence un grand nombre de connexions, mais celles-ci ne sont pas intéressantes dans le cadre de notre clusterisation, nous allons donc les supprimer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enlever les mots de moins de 2 lettres\n",
    "dat.author = dat.author.apply(lambda x: remove_short_words(x))\n",
    "dat.title = dat.title.apply(lambda x: remove_short_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après ce nettoyage, nous avons à disposition la table `dat` dont les éléments sont de la forme : `§INSERT_EXAMPLE`\n",
    "\n",
    "Nous allons maintenant descendre au niveau des mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 2 : Correspondances entre les Mots\n",
    "\n",
    "L'objectif de cette étape est de définir des taux de correspondance entre les différents mots présents dans la table `dat`. Pour réaliser cette opération, nous allons créer quatre dictionnaires représentant les quatre types de connexions possible entre deux mots A et B :\n",
    "\n",
    "    - Le mot A figure dans l'Auteur d'une ligne où B est dans le Titre\n",
    "    - Le mot A figure dans le Titre d'une ligne où B est dans l'Auteur\n",
    "    - Les mots A et B figurent dans le même Auteur sur une ligne\n",
    "    - Les mots A et B figurent dans le même Titre sur une ligne\n",
    "    \n",
    "Ces différentes connexions sont stockées respectivement dans les dictionnaires `name_word` `word_name` `name_name` et `word_word` . Elles ne sont pas directement regroupées afin de conserver la possibilité de les pondérer avec plusieurs coefficients en considérant que certaines sont plus importantes que d'autres. Cette pondération avait initialement été envisagée comme une possible amélioration en cas de résultat final non satisfaisant.\n",
    "\n",
    "Les éléments des quatre dictionnaires sont de la forme : `§INSERT_EXAMPLE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "837188it [01:47, 7785.58it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def name_word(data):\n",
    "    names = dict()\n",
    "    words = dict()\n",
    "    freqs = dict()\n",
    "    for idx, row in tqdm(data.iterrows()):\n",
    "        l_auth = row.author\n",
    "        l_tit = row.title\n",
    "        for auth in l_auth:\n",
    "            if auth not in names.keys():\n",
    "                names[auth] = [idx]\n",
    "            else:    \n",
    "                names[auth].append(idx)\n",
    "            for wd in l_tit:\n",
    "                if wd not in words.keys():\n",
    "                    words[wd] = [idx]\n",
    "                else:\n",
    "                    words[wd].append(idx)\n",
    "                \n",
    "                if auth not in freqs.keys():\n",
    "                    freqs[auth] = {wd: 1}\n",
    "                else:\n",
    "                    if wd not in freqs[auth].keys():\n",
    "                        freqs[auth][wd] = 1\n",
    "                    else:\n",
    "                        freqs[auth][wd] += 1\n",
    "\n",
    "    return names, words, freqs\n",
    "\n",
    "\n",
    "_, _, names_words = name_word(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "837188it [01:39, 8405.46it/s]\n"
     ]
    }
   ],
   "source": [
    "def word_name(data):\n",
    "    words_names = dict()\n",
    "    for idx, row in tqdm(data.iterrows()):\n",
    "        l_auth = row.author\n",
    "        l_tit = row.title\n",
    "        for word in l_tit:\n",
    "            for auth in l_auth:\n",
    "                if word not in words_names.keys():\n",
    "                    words_names[word] = {auth: 1}\n",
    "                else:\n",
    "                    if auth not in words_names[word].keys():\n",
    "                        words_names[word][auth] = 1\n",
    "                    else:\n",
    "                        words_names[word][auth] += 1\n",
    "    return words_names\n",
    "\n",
    "\n",
    "words_names = word_name(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "837188it [01:17, 10818.50it/s]\n"
     ]
    }
   ],
   "source": [
    "def name_name(data):\n",
    "    names = dict()\n",
    "    for idx, row in tqdm(data.iterrows()):\n",
    "        l_auth = row.author\n",
    "        for auth1 in l_auth:\n",
    "            for auth2 in l_auth:\n",
    "                if auth1 != auth2:\n",
    "                    if auth1 not in names.keys():\n",
    "                        names[auth1] = {auth2: 1}\n",
    "                    else:\n",
    "                        if auth2 not in names[auth1].keys():\n",
    "                            names[auth1][auth2] = 1\n",
    "                        else:\n",
    "                            names[auth1][auth2] += 1\n",
    "    return names\n",
    "\n",
    "names_names = name_name(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "837188it [01:45, 7959.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def word_word(data):\n",
    "    names = dict()\n",
    "    for idx, row in tqdm(data.iterrows()):\n",
    "        l_auth = row.title\n",
    "        for auth1 in l_auth:\n",
    "            for auth2 in l_auth:\n",
    "                if auth1 != auth2:\n",
    "                    if auth1 not in names.keys():\n",
    "                        names[auth1] = {auth2: 1}\n",
    "                    else:\n",
    "                        if auth2 not in names[auth1].keys():\n",
    "                            names[auth1][auth2] = 1\n",
    "                        else:\n",
    "                            names[auth1][auth2] += 1\n",
    "    return names\n",
    "\n",
    "words_words = word_word(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les dictionnaires créés, ils vont être rassemblés dans un graphe global dans les cellules suivantes. Celui-ci représente l'ensemble des mots différents présents dans la table initiale `dat` et toutes leurs connexions.\n",
    "\n",
    "Une opération de nettoyage est ensuite réalisé sur le graphe : on supprime toutes les connexions entre un mot et lui même grâce à la fonction `remove_diag`. Celles-ci sont en effet inutiles (chaque mot est unique dans le graphe) et pourraient potentiellement être gênantes dans les calculs suivants.\n",
    "\n",
    "Le graphe est ensuite stocké en mémoire. Il contient `§INSERT_NBRE_ELTS_GRAPH` de la forme `§INSERT_EXAMPLE` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175498/175498 [00:07<00:00, 25058.77it/s] \n",
      "100%|██████████| 126205/126205 [00:02<00:00, 50365.01it/s]\n",
      "100%|██████████| 177916/177916 [00:03<00:00, 56798.62it/s] \n"
     ]
    }
   ],
   "source": [
    "def create_graph(names_names, words_words, names_words, words_names):\n",
    "    graph = names_names.copy()\n",
    "    for word in tqdm(words_words.keys()):\n",
    "        if word not in graph.keys():\n",
    "            graph[word] = words_words[word]\n",
    "        else:\n",
    "            for wd in words_words[word].keys():\n",
    "                if wd in graph[word].keys():\n",
    "                    graph[word][wd] += words_words[word][wd]\n",
    "                else:\n",
    "                    graph[word][wd] = words_words[word][wd]\n",
    "    for word in tqdm(names_words.keys()):\n",
    "        if word not in graph.keys():\n",
    "            graph[word] = names_words[word]\n",
    "        else:\n",
    "            for wd in names_words[word].keys():\n",
    "                if wd in graph[word].keys():\n",
    "                    graph[word][wd] += names_words[word][wd]\n",
    "                else:\n",
    "                    graph[word][wd] = names_words[word][wd]\n",
    "    for word in tqdm(words_names.keys()):\n",
    "        if word not in graph.keys():\n",
    "            graph[word] = words_names[word]\n",
    "        else:\n",
    "            for wd in words_names[word].keys():\n",
    "                if wd in graph[word].keys():\n",
    "                    graph[word][wd] += words_names[word][wd]\n",
    "                else:\n",
    "                    graph[word][wd] = words_names[word][wd]\n",
    "    return graph\n",
    "\n",
    "graph = create_graph(names_names, words_words, names_words, words_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diag(graph):\n",
    "    clean_graph = dict(graph)\n",
    "    for word in clean_graph.keys():\n",
    "        if word in clean_graph[word].keys():\n",
    "            del clean_graph[word][word]\n",
    "    return clean_graph\n",
    "\n",
    "clean_graph = remove_diag(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(file=open('graph.pickle', 'wb'), obj=clean_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_weight(graph):\n",
    "    total = 0\n",
    "    for word in graph:\n",
    "        for p in graph[word]:\n",
    "            total += graph[word][p]\n",
    "    return total\n",
    "\n",
    "total = total_weight(clean_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71267172"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 3  : Réduction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce stade `clean graph` est un le graph des connexions entre un mot A et un mot B, pondéré par le nombre de connexions. Ce graph est converti en matrice d'affinité par `create_sparse_matrix`, `create_word_index` crée la table de corrspondance entre un mot et son indice dans les lignes de la matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_index(graph):\n",
    "    out = {}\n",
    "    i = 0\n",
    "    for word in graph.keys():\n",
    "        out[word] = i\n",
    "        i += 1\n",
    "    return out\n",
    "\n",
    "index = create_word_index(clean_graph)\n",
    "\n",
    "pickle.dump(file=open('index.pickle', 'wb'), obj=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 248804/248804 [00:18<00:00, 13447.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_sparse_matrix(clean_graph, index):\n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    for word in tqdm(clean_graph.keys()):\n",
    "        for wd in clean_graph[word].keys():\n",
    "            row.append(index[word])\n",
    "            col.append(index[wd])\n",
    "            data.append(clean_graph[word][wd])\n",
    "    matrix = csr_matrix((data, (row, col)))\n",
    "    return matrix\n",
    "matrix = create_sparse_matrix(graph, index)\n",
    "\n",
    "# pickle.dump(file=open('matrix.pickle', 'wb'), obj=matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. IncrementalPCA\n",
    "\n",
    "Nous avons appliqué deux algorithmes pour la réduction de dimensions, la PCA (Principal Components Analysis) et la SVD (Singular Values Decomposition). \n",
    "\n",
    "La PCA est un algorithme qui ne peut s'appliquer que sur des variables de type numpy.array. Cela pose un problème de mémoire car la matrice d'affinité est de taille `248804*248804` et ne tiens pas en mémoire. Pour palier ce problème, nous utilisons l'algorithme incremental PCA de sklearn qui permet de réaliser le calcul par batches.\n",
    "\n",
    "Le calcul a pris plus de 10h, nous metttons le fichier pickle du résultats de la PCA dans le dossier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "chunk_size = 100\n",
    "n = matrix.shape[0]\n",
    "\n",
    "pca = IncrementalPCA(n_components=15, batch_size=100)\n",
    "\n",
    "for i in tqdm(range(0, n//chunk_size)):\n",
    "    rows = matrix[i*chunk_size : (i+1)*chunk_size].toarray()\n",
    "    pca.partial_fit(rows)\n",
    "\n",
    "pickle.dump(file=open('pca.pickle', 'wb'), obj=pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import pandas as pd\n",
    "\n",
    "# pca = pickle.load(open(\"dac/pca.pickle\", 'rb'))\n",
    "# graph = pickle.load(file=open('graph.pickle', 'rb'))\n",
    "index = pickle.load(file=open('../index.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2488/2488 [15:10<00:00,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "transfo = []\n",
    "\n",
    "chunk_size = 100\n",
    "n = matrix.shape[0]\n",
    "for i in tqdm(range(0, n//chunk_size)):\n",
    "    transfo.extend(pca.transform(matrix[i*chunk_size : (i+1)*chunk_size].toarray()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(file=open('pca_transfo.pickle', 'wb'), obj=transfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TruncatedSVD\n",
    "Pour la SVD nous utilisons l'algorithme TruncatedSVD du module skelarn, il présente l'avantage de fonctionner avec des objets de type scipy.sparce_matrix, la matrice sparce d'affinité tient en mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=15, random_state=0)\n",
    "\n",
    "svd.fit(matrix)\n",
    "\n",
    "transfo_svd = svd.transform(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 4 : K-Means Clustering (au niveau des mots)\n",
    "\n",
    "A présent chaque mot du lexique est représenté par sa projection dans l'espace de dimension 'n_components', le nombre de composantes choisies pour la réduction de dimension.\n",
    "\n",
    "Dans les cellules suivantes, l'algorithme k-means avec n clusters est appliqué sur la projection des mots. Cela permet d'associer un cluster à chaque mot. Le but est ensuite de représenter les références de livres par des vecteurs de taille n et de norme 1 où chaque mot contribue pour un poids identique à la composante du cluster auquel il appartient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# transfo = pickle.load(file=open('svd_transfo.pickle', 'rb'))\n",
    "transfo = pickle.load(file=open('../pca_transfo.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=100, random_state=0)\n",
    "kmeans.fit(transfo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 210581,\n",
       "         1: 1,\n",
       "         2: 3,\n",
       "         3: 1,\n",
       "         4: 1,\n",
       "         5: 18,\n",
       "         6: 1,\n",
       "         7: 1,\n",
       "         8: 1,\n",
       "         9: 7,\n",
       "         10: 1,\n",
       "         11: 28,\n",
       "         12: 27,\n",
       "         13: 1,\n",
       "         14: 6,\n",
       "         15: 4,\n",
       "         16: 235,\n",
       "         17: 2440,\n",
       "         18: 1,\n",
       "         19: 1,\n",
       "         20: 139,\n",
       "         21: 11,\n",
       "         22: 1,\n",
       "         23: 1,\n",
       "         24: 596,\n",
       "         25: 5,\n",
       "         26: 1,\n",
       "         27: 37,\n",
       "         28: 3,\n",
       "         29: 3,\n",
       "         30: 8,\n",
       "         31: 1,\n",
       "         32: 1,\n",
       "         33: 2,\n",
       "         34: 33,\n",
       "         35: 1,\n",
       "         36: 33,\n",
       "         37: 2095,\n",
       "         38: 1057,\n",
       "         39: 47,\n",
       "         40: 1,\n",
       "         41: 2,\n",
       "         42: 8,\n",
       "         43: 22,\n",
       "         44: 10,\n",
       "         45: 24,\n",
       "         46: 4,\n",
       "         47: 8,\n",
       "         48: 1,\n",
       "         49: 64,\n",
       "         50: 2,\n",
       "         51: 6,\n",
       "         52: 149,\n",
       "         53: 43,\n",
       "         54: 2,\n",
       "         55: 1,\n",
       "         56: 5133,\n",
       "         57: 7,\n",
       "         58: 232,\n",
       "         59: 2,\n",
       "         60: 1,\n",
       "         61: 11,\n",
       "         62: 20041,\n",
       "         63: 157,\n",
       "         64: 100,\n",
       "         65: 1,\n",
       "         66: 9,\n",
       "         67: 103,\n",
       "         68: 14,\n",
       "         69: 72,\n",
       "         70: 3,\n",
       "         71: 177,\n",
       "         72: 32,\n",
       "         73: 1,\n",
       "         74: 5,\n",
       "         75: 1,\n",
       "         76: 113,\n",
       "         77: 1,\n",
       "         78: 1,\n",
       "         79: 1,\n",
       "         80: 2,\n",
       "         81: 7,\n",
       "         82: 1450,\n",
       "         83: 1,\n",
       "         84: 91,\n",
       "         85: 24,\n",
       "         86: 3,\n",
       "         87: 484,\n",
       "         88: 172,\n",
       "         89: 1,\n",
       "         90: 7,\n",
       "         91: 27,\n",
       "         92: 9,\n",
       "         93: 20,\n",
       "         94: 22,\n",
       "         95: 1,\n",
       "         96: 358,\n",
       "         97: 2071,\n",
       "         98: 50,\n",
       "         99: 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "a = Counter(kmeans.labels_)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui à chaque mot associe un cluster\n",
    "\n",
    "def create_clusters_index(index, clusters):\n",
    "    clusters_index = {}\n",
    "    for word, row in index.items():\n",
    "        if row < len(clusters):\n",
    "            clusters_index[word] = clusters[row]\n",
    "    return clusters_index\n",
    "\n",
    "clusters_index = create_clusters_index(index, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 5 : K-Means Clustering (au niveau des livres)\n",
    "\n",
    "A ce stade, les mots sont répartis dans n clusters. Chaque mot a donc une étiquette de 0 à n-1.\n",
    "\n",
    "Dans cette étape, les références de livres sont converties en vecteurs de taille n. Chaque mot contribue pour le même poids à la composante de son étiquette cluster. La représentation vectorielle d'un mot est ensuite normalisée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def book_to_vect(clusters_index, n_clusters, book_author, book_title):\n",
    "    \"\"\"\n",
    "    kmeans is an index of kmeans[word] = cluster_id\n",
    "    \"\"\"\n",
    "    vect = np.zeros(n_clusters)\n",
    "    for word in book_author:\n",
    "        if word in clusters_index.keys():\n",
    "            vect[clusters_index[word]] += 1\n",
    "    for word in book_title:\n",
    "        if word in clusters_index.keys():\n",
    "            vect[clusters_index[word]] += 1\n",
    "    norm = np.linalg.norm(vect)\n",
    "    if norm:\n",
    "        vect = vect/np.linalg.norm(vect)\n",
    "    return vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['coordinates'] = [[0]*100]*len(dat)\n",
    "\n",
    "dat['coordinates'] = dat.apply(lambda x: book_to_vect(clusters_index, 100, x['author'], x['title']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(file=open('vectorized_db.pickle', 'wb'), obj=dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vectorized DB with PCA (15) and kmeans (100)\n",
    "\n",
    "dat = pickle.load(file=open('../vectorized_db.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce stade chaque référence de livre est représentée par un vecteur de dimansion n=100.\n",
    "\n",
    "L'ultime étape consiste à appliquer un algorithme de clustering sur les références. Le choix du nombre de clusters dépend du problème à résoudre. \n",
    "\n",
    "Les cellules suivantes résolvent le problème avec 10 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=10, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=0, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_books = KMeans(n_clusters=10, random_state=0)\n",
    "kmeans_books.fit(dat.coordinates.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 61448,\n",
       "         1: 76613,\n",
       "         2: 58604,\n",
       "         3: 95878,\n",
       "         4: 77744,\n",
       "         5: 109577,\n",
       "         6: 84185,\n",
       "         7: 73933,\n",
       "         8: 79142,\n",
       "         9: 120064})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "b = Counter(kmeans_books.labels_)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['cluster'] = kmeans_books.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[lynda, curnyn]</td>\n",
       "      <td>[operation, bague, doigt]</td>\n",
       "      <td>[0.4472135954999579, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[riche, pierre]</td>\n",
       "      <td>[education, culture, dans, occident, bar]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.3779644730092272, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[berriot]</td>\n",
       "      <td>[julio, cortazar, enchanteur]</td>\n",
       "      <td>[0.4082482904638631, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[hubert, monteilhet]</td>\n",
       "      <td>[neropolis, roman, des, temps, neroniens]</td>\n",
       "      <td>[0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>[bernard, kouchner]</td>\n",
       "      <td>[que, crois]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>[raymond, moody]</td>\n",
       "      <td>[vie, apres, vie]</td>\n",
       "      <td>[0.0, 0.0, 0.7559289460184544, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>[caroline, cooney]</td>\n",
       "      <td>[photo, jenny, spring]</td>\n",
       "      <td>[0.4472135954999579, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>[barbara, pease, allan, pease]</td>\n",
       "      <td>[pourquoi, les, hommes, mentent, les, femmes, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.48507125007266594, 0.0, 0.0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[isabel, allende]</td>\n",
       "      <td>[maison, aux, esprits]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>[pierre, jean, jouve]</td>\n",
       "      <td>[scene, capitale]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>[francoise, dolto, gerard, severin]</td>\n",
       "      <td>[foi, risque, psychanalyse]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>[pierre, luc, seguillon]</td>\n",
       "      <td>[portraits, domicile]</td>\n",
       "      <td>[0.4472135954999579, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>[michel, jeury]</td>\n",
       "      <td>[cabaret, des, oiseaux]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4472135954999...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>[depardieu, gerard]</td>\n",
       "      <td>[lettres, volees]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>[clive, cussler]</td>\n",
       "      <td>[panique, maison, blanche]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>[graham, swift]</td>\n",
       "      <td>[dimanche, des, meres]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4472135954999...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[francoise, dolto]</td>\n",
       "      <td>[sexualite, feminine]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>[john, grisham]</td>\n",
       "      <td>[firme]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>[vittorio, saltini]</td>\n",
       "      <td>[livre]</td>\n",
       "      <td>[0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>[pagnol, marcel]</td>\n",
       "      <td>[topaze]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>[eliette, abecassis]</td>\n",
       "      <td>[qumran]</td>\n",
       "      <td>[0.4472135954999579, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>[alison, mcleay]</td>\n",
       "      <td>[par, vents, marees]</td>\n",
       "      <td>[0.4472135954999579, 0.0, 0.0, 0.0, 0.44721359...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>[paulo, coelho]</td>\n",
       "      <td>[maktub]</td>\n",
       "      <td>[0.4472135954999579, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>[sylviane, agacinski]</td>\n",
       "      <td>[journal, interrompu, janvier, mai, 2002]</td>\n",
       "      <td>[0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>[philippe, delmas]</td>\n",
       "      <td>[maitre, des, horloges]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4472135954999...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>[gaston, leroux]</td>\n",
       "      <td>[parfum, dame, noir]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>[prevot, georges]</td>\n",
       "      <td>[cooperation, scolaire, pedagogie]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>[patrick, segal]</td>\n",
       "      <td>[reve, sais]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>[judy, blume]</td>\n",
       "      <td>[super, sheila, neuf]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>[driss, chraibi]</td>\n",
       "      <td>[inspecteur, ali, trinity, college]</td>\n",
       "      <td>[0.31622776601683794, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014763</th>\n",
       "      <td>[rouet, marcel]</td>\n",
       "      <td>[maigrir, par, gymnastique]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.4472135954999579, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014765</th>\n",
       "      <td>[marcel, rouet]</td>\n",
       "      <td>[maigrir, par, gymnastique]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.4472135954999579, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014766</th>\n",
       "      <td>[rouet, marcel]</td>\n",
       "      <td>[maigrir, par, gymnastique]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.4472135954999579, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014767</th>\n",
       "      <td>[marcel, rouet]</td>\n",
       "      <td>[marcel, rouet, maigrir, eliminez, cellulite, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.1796053020267749, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014768</th>\n",
       "      <td>[marcel, rouet]</td>\n",
       "      <td>[maigrir, par, gymnastique, les, guides, prati...</td>\n",
       "      <td>[0.0, 0.0, 0.31622776601683794, 0.316227766016...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014811</th>\n",
       "      <td>[alfredo, gonzalez, hermoso]</td>\n",
       "      <td>[espagnol, poche]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014819</th>\n",
       "      <td>[francois, joly]</td>\n",
       "      <td>[bop, lola]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014860</th>\n",
       "      <td>[moody, raymond]</td>\n",
       "      <td>[vie, apres, vie]</td>\n",
       "      <td>[0.0, 0.0, 0.7559289460184544, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014861</th>\n",
       "      <td>[moody, raymond]</td>\n",
       "      <td>[vie, apres, vie]</td>\n",
       "      <td>[0.0, 0.0, 0.7559289460184544, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014872</th>\n",
       "      <td>[jacques, guillet]</td>\n",
       "      <td>[jesus, devant, vie, mort]</td>\n",
       "      <td>[0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014881</th>\n",
       "      <td>[bouchet, christophe]</td>\n",
       "      <td>[chantier, siecle, tunnel, sous, manche]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014895</th>\n",
       "      <td>[cau, jean]</td>\n",
       "      <td>[pape, est, mort, 1968, broche, 153, pages, ma...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014956</th>\n",
       "      <td>[geronimo, stilton]</td>\n",
       "      <td>[geronimo, stilton, tome, rejoue, nous, mozart]</td>\n",
       "      <td>[0.2886751345948129, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014972</th>\n",
       "      <td>[connor, dayton]</td>\n",
       "      <td>[memorial, day, american, holidays, powerkids,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014975</th>\n",
       "      <td>[pieter, van, oudheusden, collectif]</td>\n",
       "      <td>[lis, mes, premieres, histoires]</td>\n",
       "      <td>[0.35355339059327373, 0.35355339059327373, 0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014982</th>\n",
       "      <td>[geronimo, stilton]</td>\n",
       "      <td>[dur, dur, etre, une, super, souris]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.316...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014988</th>\n",
       "      <td>[jean, pierre, andrevon]</td>\n",
       "      <td>[visiteur, anti, monde]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014991</th>\n",
       "      <td>[catherine, dolto, colline, faure, poiree]</td>\n",
       "      <td>[propre]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014998</th>\n",
       "      <td>[tony, ross, jeanne, willis]</td>\n",
       "      <td>[veux, etre, une, cow, girl]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.301...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015008</th>\n",
       "      <td>[graham, reynolds]</td>\n",
       "      <td>[turner, world, art]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015009</th>\n",
       "      <td>[graham, reynolds]</td>\n",
       "      <td>[turner]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015035</th>\n",
       "      <td>[finzo, hubert, ben, kemoun]</td>\n",
       "      <td>[mes, monstres, moi]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015050</th>\n",
       "      <td>[john, grisham]</td>\n",
       "      <td>[non, coupable]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015052</th>\n",
       "      <td>[graham, greene]</td>\n",
       "      <td>[americain, bien, tranquille]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015076</th>\n",
       "      <td>[petit, macagno]</td>\n",
       "      <td>[mathematiques, les, cahiers, vacances, method...</td>\n",
       "      <td>[0.2773500981126146, 0.0, 0.0, 0.2773500981126...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015078</th>\n",
       "      <td>[petit, macagno]</td>\n",
       "      <td>[mathematiques, les, cahiers, vacances, method...</td>\n",
       "      <td>[0.4472135954999579, 0.0, 0.0, 0.2236067977499...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015079</th>\n",
       "      <td>[thomas, petit]</td>\n",
       "      <td>[les, vacances, method, maths, premiere, termi...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.2886751345948129, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015081</th>\n",
       "      <td>[thomas, petit]</td>\n",
       "      <td>[les, vacances, method, maths, terminale, aux,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.23570226039551587, 0.0, 0.0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015082</th>\n",
       "      <td>[thomas, petit]</td>\n",
       "      <td>[les, vacances, method, maths, terminale, aux,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.24253562503633297, 0.0, 0.0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015083</th>\n",
       "      <td>[marie, neuser]</td>\n",
       "      <td>[petit, jouet, mecanique]</td>\n",
       "      <td>[0.4472135954999579, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61448 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             author  \\\n",
       "7                                   [lynda, curnyn]   \n",
       "33                                  [riche, pierre]   \n",
       "36                                        [berriot]   \n",
       "41                             [hubert, monteilhet]   \n",
       "60                              [bernard, kouchner]   \n",
       "68                                 [raymond, moody]   \n",
       "75                               [caroline, cooney]   \n",
       "94                   [barbara, pease, allan, pease]   \n",
       "123                               [isabel, allende]   \n",
       "141                           [pierre, jean, jouve]   \n",
       "149             [francoise, dolto, gerard, severin]   \n",
       "158                        [pierre, luc, seguillon]   \n",
       "181                                 [michel, jeury]   \n",
       "185                             [depardieu, gerard]   \n",
       "195                                [clive, cussler]   \n",
       "196                                 [graham, swift]   \n",
       "199                              [francoise, dolto]   \n",
       "201                                 [john, grisham]   \n",
       "203                             [vittorio, saltini]   \n",
       "207                                [pagnol, marcel]   \n",
       "217                            [eliette, abecassis]   \n",
       "238                                [alison, mcleay]   \n",
       "245                                 [paulo, coelho]   \n",
       "259                           [sylviane, agacinski]   \n",
       "271                              [philippe, delmas]   \n",
       "276                                [gaston, leroux]   \n",
       "277                               [prevot, georges]   \n",
       "291                                [patrick, segal]   \n",
       "293                                   [judy, blume]   \n",
       "296                                [driss, chraibi]   \n",
       "...                                             ...   \n",
       "1014763                             [rouet, marcel]   \n",
       "1014765                             [marcel, rouet]   \n",
       "1014766                             [rouet, marcel]   \n",
       "1014767                             [marcel, rouet]   \n",
       "1014768                             [marcel, rouet]   \n",
       "1014811                [alfredo, gonzalez, hermoso]   \n",
       "1014819                            [francois, joly]   \n",
       "1014860                            [moody, raymond]   \n",
       "1014861                            [moody, raymond]   \n",
       "1014872                          [jacques, guillet]   \n",
       "1014881                       [bouchet, christophe]   \n",
       "1014895                                 [cau, jean]   \n",
       "1014956                         [geronimo, stilton]   \n",
       "1014972                            [connor, dayton]   \n",
       "1014975        [pieter, van, oudheusden, collectif]   \n",
       "1014982                         [geronimo, stilton]   \n",
       "1014988                    [jean, pierre, andrevon]   \n",
       "1014991  [catherine, dolto, colline, faure, poiree]   \n",
       "1014998                [tony, ross, jeanne, willis]   \n",
       "1015008                          [graham, reynolds]   \n",
       "1015009                          [graham, reynolds]   \n",
       "1015035                [finzo, hubert, ben, kemoun]   \n",
       "1015050                             [john, grisham]   \n",
       "1015052                            [graham, greene]   \n",
       "1015076                            [petit, macagno]   \n",
       "1015078                            [petit, macagno]   \n",
       "1015079                             [thomas, petit]   \n",
       "1015081                             [thomas, petit]   \n",
       "1015082                             [thomas, petit]   \n",
       "1015083                             [marie, neuser]   \n",
       "\n",
       "                                                     title  \\\n",
       "7                                [operation, bague, doigt]   \n",
       "33               [education, culture, dans, occident, bar]   \n",
       "36                           [julio, cortazar, enchanteur]   \n",
       "41               [neropolis, roman, des, temps, neroniens]   \n",
       "60                                            [que, crois]   \n",
       "68                                       [vie, apres, vie]   \n",
       "75                                  [photo, jenny, spring]   \n",
       "94       [pourquoi, les, hommes, mentent, les, femmes, ...   \n",
       "123                                 [maison, aux, esprits]   \n",
       "141                                      [scene, capitale]   \n",
       "149                            [foi, risque, psychanalyse]   \n",
       "158                                  [portraits, domicile]   \n",
       "181                                [cabaret, des, oiseaux]   \n",
       "185                                      [lettres, volees]   \n",
       "195                             [panique, maison, blanche]   \n",
       "196                                 [dimanche, des, meres]   \n",
       "199                                  [sexualite, feminine]   \n",
       "201                                                [firme]   \n",
       "203                                                [livre]   \n",
       "207                                               [topaze]   \n",
       "217                                               [qumran]   \n",
       "238                                   [par, vents, marees]   \n",
       "245                                               [maktub]   \n",
       "259              [journal, interrompu, janvier, mai, 2002]   \n",
       "271                                [maitre, des, horloges]   \n",
       "276                                   [parfum, dame, noir]   \n",
       "277                     [cooperation, scolaire, pedagogie]   \n",
       "291                                           [reve, sais]   \n",
       "293                                  [super, sheila, neuf]   \n",
       "296                    [inspecteur, ali, trinity, college]   \n",
       "...                                                    ...   \n",
       "1014763                        [maigrir, par, gymnastique]   \n",
       "1014765                        [maigrir, par, gymnastique]   \n",
       "1014766                        [maigrir, par, gymnastique]   \n",
       "1014767  [marcel, rouet, maigrir, eliminez, cellulite, ...   \n",
       "1014768  [maigrir, par, gymnastique, les, guides, prati...   \n",
       "1014811                                  [espagnol, poche]   \n",
       "1014819                                        [bop, lola]   \n",
       "1014860                                  [vie, apres, vie]   \n",
       "1014861                                  [vie, apres, vie]   \n",
       "1014872                         [jesus, devant, vie, mort]   \n",
       "1014881           [chantier, siecle, tunnel, sous, manche]   \n",
       "1014895  [pape, est, mort, 1968, broche, 153, pages, ma...   \n",
       "1014956    [geronimo, stilton, tome, rejoue, nous, mozart]   \n",
       "1014972  [memorial, day, american, holidays, powerkids,...   \n",
       "1014975                   [lis, mes, premieres, histoires]   \n",
       "1014982               [dur, dur, etre, une, super, souris]   \n",
       "1014988                            [visiteur, anti, monde]   \n",
       "1014991                                           [propre]   \n",
       "1014998                       [veux, etre, une, cow, girl]   \n",
       "1015008                               [turner, world, art]   \n",
       "1015009                                           [turner]   \n",
       "1015035                               [mes, monstres, moi]   \n",
       "1015050                                    [non, coupable]   \n",
       "1015052                      [americain, bien, tranquille]   \n",
       "1015076  [mathematiques, les, cahiers, vacances, method...   \n",
       "1015078  [mathematiques, les, cahiers, vacances, method...   \n",
       "1015079  [les, vacances, method, maths, premiere, termi...   \n",
       "1015081  [les, vacances, method, maths, terminale, aux,...   \n",
       "1015082  [les, vacances, method, maths, terminale, aux,...   \n",
       "1015083                          [petit, jouet, mecanique]   \n",
       "\n",
       "                                               coordinates  cluster  \n",
       "7        [0.4472135954999579, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "33       [0.0, 0.0, 0.0, 0.0, 0.0, 0.3779644730092272, ...        0  \n",
       "36       [0.4082482904638631, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "41       [0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "60       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "68       [0.0, 0.0, 0.7559289460184544, 0.0, 0.0, 0.0, ...        0  \n",
       "75       [0.4472135954999579, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "94       [0.0, 0.0, 0.0, 0.48507125007266594, 0.0, 0.0,...        0  \n",
       "123      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "141      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "149      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "158      [0.4472135954999579, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "181      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4472135954999...        0  \n",
       "185      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "195      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "196      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4472135954999...        0  \n",
       "199      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "201      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "203      [0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "207      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "217      [0.4472135954999579, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "238      [0.4472135954999579, 0.0, 0.0, 0.0, 0.44721359...        0  \n",
       "245      [0.4472135954999579, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "259      [0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "271      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4472135954999...        0  \n",
       "276      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "277      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "291      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "293      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "296      [0.31622776601683794, 0.0, 0.0, 0.0, 0.0, 0.0,...        0  \n",
       "...                                                    ...      ...  \n",
       "1014763  [0.0, 0.0, 0.0, 0.0, 0.4472135954999579, 0.0, ...        0  \n",
       "1014765  [0.0, 0.0, 0.0, 0.0, 0.4472135954999579, 0.0, ...        0  \n",
       "1014766  [0.0, 0.0, 0.0, 0.0, 0.4472135954999579, 0.0, ...        0  \n",
       "1014767  [0.0, 0.0, 0.0, 0.0, 0.1796053020267749, 0.0, ...        0  \n",
       "1014768  [0.0, 0.0, 0.31622776601683794, 0.316227766016...        0  \n",
       "1014811  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "1014819  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "1014860  [0.0, 0.0, 0.7559289460184544, 0.0, 0.0, 0.0, ...        0  \n",
       "1014861  [0.0, 0.0, 0.7559289460184544, 0.0, 0.0, 0.0, ...        0  \n",
       "1014872  [0.0, 0.0, 0.7071067811865475, 0.0, 0.0, 0.0, ...        0  \n",
       "1014881  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "1014895  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "1014956  [0.2886751345948129, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "1014972  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "1014975  [0.35355339059327373, 0.35355339059327373, 0.0...        0  \n",
       "1014982  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.316...        0  \n",
       "1014988  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "1014991  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "1014998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.301...        0  \n",
       "1015008  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "1015009  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "1015035  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "1015050  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "1015052  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "1015076  [0.2773500981126146, 0.0, 0.0, 0.2773500981126...        0  \n",
       "1015078  [0.4472135954999579, 0.0, 0.0, 0.2236067977499...        0  \n",
       "1015079  [0.0, 0.0, 0.0, 0.2886751345948129, 0.0, 0.0, ...        0  \n",
       "1015081  [0.0, 0.0, 0.0, 0.23570226039551587, 0.0, 0.0,...        0  \n",
       "1015082  [0.0, 0.0, 0.0, 0.24253562503633297, 0.0, 0.0,...        0  \n",
       "1015083  [0.4472135954999579, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0  \n",
       "\n",
       "[61448 rows x 4 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[dat['cluster'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.merge(data, dat, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 6 : Résultats et Vérifications\n",
    "\n",
    "Dans cette partie, la pertinence de la vectorisation des références de livres est évaluée. Le test est effectué sur un ensemble de test qui comporte 5000 références auxquelles ont été attribuées à la main des clusters. Les références sont vectorisées à l'aide du modèle développé au dessus et différents algorithmes de clustering sont testés sur cette représentation. La performance est mesurée en terme de rappel, précision et F1_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASIN</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>PARUTION_ID</th>\n",
       "      <th>PUBLICATION_DATE</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>id_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2221075943</td>\n",
       "      <td>Mark Childress</td>\n",
       "      <td>2221075943</td>\n",
       "      <td>18649.0</td>\n",
       "      <td>1995-02-28</td>\n",
       "      <td>Robert Laffont</td>\n",
       "      <td>La tête dans le carton à chapeaux</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2353251684</td>\n",
       "      <td>Claire Bigard</td>\n",
       "      <td>2353251684</td>\n",
       "      <td>257114.0</td>\n",
       "      <td>2009-11-19</td>\n",
       "      <td>Editions Clair de Lune</td>\n",
       "      <td>Méprise, Tome 2 :</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2736108779</td>\n",
       "      <td>Marvin Bryan</td>\n",
       "      <td>2736108779</td>\n",
       "      <td>316861.0</td>\n",
       "      <td>1991</td>\n",
       "      <td>Sybex</td>\n",
       "      <td>Macintosh Système 7 : mode d'emploi</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2744142026</td>\n",
       "      <td>Luanne Rice Francine Siéty</td>\n",
       "      <td>2744142026</td>\n",
       "      <td>272683.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>France loisirs</td>\n",
       "      <td>Sarah</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2702130895</td>\n",
       "      <td>J. Deaver</td>\n",
       "      <td>2702130895</td>\n",
       "      <td>257270.0</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>Calmann-Lévy</td>\n",
       "      <td>Bone Collector</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ASIN                       AUTHOR        ISBN PARUTION_ID  \\\n",
       "0  2221075943               Mark Childress  2221075943     18649.0   \n",
       "1  2353251684                Claire Bigard  2353251684    257114.0   \n",
       "2  2736108779                 Marvin Bryan  2736108779    316861.0   \n",
       "3  2744142026   Luanne Rice Francine Siéty  2744142026    272683.0   \n",
       "4  2702130895                    J. Deaver  2702130895    257270.0   \n",
       "\n",
       "  PUBLICATION_DATE               PUBLISHER  \\\n",
       "0       1995-02-28          Robert Laffont   \n",
       "1       2009-11-19  Editions Clair de Lune   \n",
       "2             1991                   Sybex   \n",
       "3             2001          France loisirs   \n",
       "4       2000-01-05            Calmann-Lévy   \n",
       "\n",
       "                                  TITLE  id_cluster  \n",
       "0     La tête dans le carton à chapeaux          45  \n",
       "1                     Méprise, Tome 2 :          47  \n",
       "2   Macintosh Système 7 : mode d'emploi          49  \n",
       "3                                 Sarah          78  \n",
       "4                        Bone Collector          72  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chargement du test_set\n",
    "data = pd.read_csv(\"../test_set.csv\", index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "import pre_processing.cleaning\n",
    "import pre_processing.volume_number_detection\n",
    "data['author'] = pre_processing.cleaning.clean_df_column(data.AUTHOR)\n",
    "data['title'] = pre_processing.cleaning.clean_df_column(data.TITLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.author = data.author.apply(lambda x: remove_short_words(x))\n",
    "data.title = data.title.apply(lambda x: remove_short_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['coordinates'] = [[0]*100]*len(data)\n",
    "\n",
    "data['coordinates'] = data.apply(lambda x: book_to_vect(clusters_index, 100, x['author'], x['title']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. clustering avec Kmeans\n",
    "\n",
    "Pour le kmeans, il est supposé que le nombre k est connu a priori. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_books = KMeans(n_clusters=4743, random_state=0)\n",
    "kmeans_books.fit(new_data.coordinates.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['id_cluster'] = kmeans_books.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.reset_index(inplace=True)\n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.rename({'index': 'id'}, inplace=True, axis=1)\n",
    "data.rename({'index': 'id'}, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scoring.scoring\n",
    "precision, recall, F_score = scoring.scoring.performance(new_data, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2923433874709977\n",
      "0.40384615384615385\n",
      "0.33916554508748326\n"
     ]
    }
   ],
   "source": [
    "print(precision)\n",
    "print(recall)\n",
    "print(F_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. clustering avec DBSCAN\n",
    "\n",
    "Cet algorithme a l'avantage de ne pas nécessiter la connaissance a priori d'une information à propos des clusters. Il ne permet pas d'obtenir de neilleures performances que Kmeans, même avec du tuning de paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan_books = DBSCAN(eps=0.6, min_samples=1, n_jobs=-1)\n",
    "labels = dbscan_books.fit_predict(new_data.coordinates.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['id_cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00024055306286802005\n",
      "0.6634615384615384\n",
      "0.0004809317529962397\n"
     ]
    }
   ],
   "source": [
    "import scoring.scoring\n",
    "precision, recall, F_score = scoring.scoring.performance(new_data, data)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(F_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interprétation\n",
    "\n",
    "Lors du clustering des mots, la majorité des mots (plus de 200000) se retrouvent dans le même cluster, ils sont mal répartis par l'algorithme. \n",
    "\n",
    "Lors du passage à la représentation vectorielle des références de livre, un grand nombre de livres a priori différents se touve rapproché dans l'espace et un algorithmes de clustering peine à les distinguer. \n",
    "\n",
    "Une solution pour améliorer les performances pourrait être d'augmenter la dimension de sortie de la PCA/SVD afin de garder plus d'information descriptive des mots. Aussi certains mots apparaissent dans un grand nombre de références (parfois plus de 1% des références). Cela crée de nombreuses connexions dans la matrice d'affinité, or ces connexions ne sont pas riches en information car elles sont faites avec des mots qui sont fortment connecté. On pourrait supprimer les mots trop communs, au même titre que les mots de moins de 2 lettre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
